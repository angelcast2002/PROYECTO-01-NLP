{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcd26cf0",
   "metadata": {},
   "source": [
    "# Descarga del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9ee062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\caste\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: dependencias listas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess, pkgutil\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs)\n",
    "\n",
    "req = [\"pandas\",\"numpy\",\"scikit-learn\",\"matplotlib\",\"nltk\",\"gensim\",\"textdistance\",\"spacy\",\"datasets\"]\n",
    "to_install = [p for p in req if pkgutil.find_loader(p) is None]\n",
    "if to_install:\n",
    "    pip_install(to_install)\n",
    "\n",
    "try:\n",
    "    import es_core_news_sm\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"es_core_news_sm\"])\n",
    "\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "print(\"OK: dependencias listas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75d0f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas: ['language', 'category', 'newspaper', 'hash', 'text']\n",
      "Listo -> data/raw/dataset.csv (10200, 2)\n",
      "                                                text label\n",
      "0  Valladolid misteriosa es el título del nuevo l...  play\n",
      "1  El coraje de ser, de Mónica Cavallé, la aventu...  play\n",
      "2  En la Tercera el francés, de Federico Supervie...  play\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, os\n",
    "\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "df = pd.read_csv(\"data/raw/data.csv\")\n",
    "\n",
    "print(\"Columnas:\", df.columns.tolist())\n",
    "# Intento automático de mapear columnas comunes\n",
    "text_candidates  = [c for c in [\"text\",\"content\",\"article\",\"body\",\"news\",\"texto\"] if c in df.columns]\n",
    "label_candidates = [c for c in [\"label\",\"category\",\"topic\",\"class\",\"etiqueta\"] if c in df.columns]\n",
    "\n",
    "if not text_candidates or not label_candidates:\n",
    "    raise ValueError(\"No encuentro columnas de texto/etiqueta. Dime qué columnas trae y lo ajusto.\")\n",
    "\n",
    "TEXT_COL  = text_candidates[0]\n",
    "LABEL_COL = label_candidates[0]\n",
    "\n",
    "out = df[[TEXT_COL, LABEL_COL]].rename(columns={TEXT_COL:\"text\", LABEL_COL:\"label\"})\n",
    "out.to_csv(\"data/raw/dataset.csv\", index=False)\n",
    "print(\"Listo -> data/raw/dataset.csv\", out.shape)\n",
    "print(out.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61319f80",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e736f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK deps. spaCy: on\n"
     ]
    }
   ],
   "source": [
    "# Celda 1 — Dependencias mínimas\n",
    "import sys, subprocess, pkgutil\n",
    "\n",
    "def ensure(pkg, pip_name=None):\n",
    "    pip_name = pip_name or pkg\n",
    "    if pkgutil.find_loader(pkg) is None:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pip_name])\n",
    "\n",
    "ensure(\"pandas\")\n",
    "ensure(\"numpy\")\n",
    "ensure(\"nltk\")\n",
    "ensure(\"textdistance\")\n",
    "try:\n",
    "    ensure(\"spacy\")\n",
    "    import spacy\n",
    "    try:\n",
    "        import es_core_news_sm  # modelo pequeño\n",
    "        nlp = es_core_news_sm.load()\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"es_core_news_sm\"])\n",
    "        import es_core_news_sm\n",
    "        nlp = es_core_news_sm.load()\n",
    "except Exception:\n",
    "    nlp = None\n",
    "\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "print(\"OK deps. spaCy:\", \"on\" if nlp else \"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428131e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10200, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Valladolid misteriosa es el título del nuevo l...</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El coraje de ser, de Mónica Cavallé, la aventu...</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>En la Tercera el francés, de Federico Supervie...</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  Valladolid misteriosa es el título del nuevo l...  play\n",
       "1  El coraje de ser, de Mónica Cavallé, la aventu...  play\n",
       "2  En la Tercera el francés, de Federico Supervie...  play"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Celda 2 — Cargar dataset local\n",
    "import pandas as pd, os\n",
    "\n",
    "path = \"data/raw/dataset.csv\"  # del paso anterior\n",
    "df = pd.read_csv(path)\n",
    "assert {\"text\",\"label\"}.issubset(df.columns), df.columns.tolist()\n",
    "\n",
    "df = df.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 3 — Funciones: normalizar, tokenizar, limpiar\n",
    "import re, unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "SPANISH_SW = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "def normalize(txt:str)->str:\n",
    "    txt = txt.lower()\n",
    "    txt = \"\".join(c for c in unicodedata.normalize(\"NFD\", txt) if unicodedata.category(c) != \"Mn\")\n",
    "    txt = re.sub(r\"[\\r\\n\\t]+\",\" \", txt)\n",
    "    txt = re.sub(r\"\\s+\",\" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "def tokenize_simple(txt:str):\n",
    "    # tokens alfanuméricos (rápido y suficiente para el pipeline base)\n",
    "    return re.findall(r\"\\b\\w+\\b\", txt, flags=re.UNICODE)\n",
    "\n",
    "def clean_tokens(tokens, remove_digits=True, remove_sw=True):\n",
    "    out=[]\n",
    "    for t in tokens:\n",
    "        if remove_digits and t.isdigit(): \n",
    "            continue\n",
    "        if remove_sw and t in SPANISH_SW:\n",
    "            continue\n",
    "        out.append(t)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb896419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Valladolid misteriosa es el título del nuevo l...   \n",
      "1  El coraje de ser, de Mónica Cavallé, la aventu...   \n",
      "\n",
      "                                           text_norm  \n",
      "0  valladolid misteriosa es el titulo del nuevo l...  \n",
      "1  el coraje de ser, de monica cavalle, la aventu...  \n",
      "Ejemplo tokens: ['valladolid', 'misteriosa', 'titulo', 'nuevo', 'libro', 'acaba', 'publicar', 'editorial', 'almuzara', 'trata', 'obra', 'investigador', 'paranormal', 'juan', 'carlos', 'pasalodos', 'perez', 'realiza', 'exhaustiva', 'recopilacion']\n"
     ]
    }
   ],
   "source": [
    "# Celda 4 — Aplicar normalización + tokenización + limpieza\n",
    "df[\"text_norm\"] = df[\"text\"].map(normalize)\n",
    "df[\"tokens\"] = df[\"text_norm\"].map(tokenize_simple).map(clean_tokens)\n",
    "\n",
    "print(df[[\"text\", \"text_norm\"]].head(2))  # vista rápida\n",
    "print(\"Ejemplo tokens:\", df[\"tokens\"].iloc[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cc0775d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>stems</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[valladolid, misteriosa, titulo, nuevo, libro,...</td>\n",
       "      <td>[valladol, misteri, titul, nuev, libr, acab, p...</td>\n",
       "      <td>[valladolid, misterioso, titulo, nuevo, libro,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[coraje, ser, monica, cavalle, aventura, autoc...</td>\n",
       "      <td>[coraj, ser, monic, cavall, aventur, autoconoc...</td>\n",
       "      <td>[coraje, ser, monicar, cavalle, aventura, auto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [valladolid, misteriosa, titulo, nuevo, libro,...   \n",
       "1  [coraje, ser, monica, cavalle, aventura, autoc...   \n",
       "\n",
       "                                               stems  \\\n",
       "0  [valladol, misteri, titul, nuev, libr, acab, p...   \n",
       "1  [coraj, ser, monic, cavall, aventur, autoconoc...   \n",
       "\n",
       "                                              lemmas  \n",
       "0  [valladolid, misterioso, titulo, nuevo, libro,...  \n",
       "1  [coraje, ser, monicar, cavalle, aventura, auto...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Celda 5 — Stemming (Snowball) y Lemmatización (si spaCy disponible)\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "stemmer = SpanishStemmer()\n",
    "\n",
    "df[\"stems\"] = df[\"tokens\"].map(lambda toks: [stemmer.stem(t) for t in toks])\n",
    "\n",
    "if nlp is not None:\n",
    "    def lemmatize_list(toks):\n",
    "        doc = nlp(\" \".join(toks))\n",
    "        return [t.lemma_ if t.lemma_ else t.text for t in doc]\n",
    "    df[\"lemmas\"] = df[\"tokens\"].map(lemmatize_list)\n",
    "else:\n",
    "    df[\"lemmas\"] = df[\"tokens\"]  # fallback\n",
    "\n",
    "df[[\"tokens\",\"stems\",\"lemmas\"]].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6628fa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arcanos -> ['arcano']\n",
      "asentamos -> ['sentamos', 'asentados']\n",
      "yoes -> ['oes', 'goes', 'yves', 'does']\n",
      "llenados -> ['llegados', 'llenamos', 'llenado', 'llevados']\n",
      "azores -> ['amores']\n"
     ]
    }
   ],
   "source": [
    "# Celda 6 — Levenshtein: sugerencias para tokens raros\n",
    "from collections import Counter\n",
    "import textdistance\n",
    "\n",
    "# vocabulario y raros (freq==1). Ajusta si quieres más/menos.\n",
    "vocab = Counter(t for toks in df[\"tokens\"] for t in toks)\n",
    "rare = [w for w,c in vocab.items() if c==1][:100]     # muestra de raros\n",
    "cand = set([w for w,c in vocab.items() if c>=5])      # candidatos \"correctos\"\n",
    "\n",
    "def suggest(word, candidates, max_dist=1):\n",
    "    # devuelve candidatos a distancia <= max_dist\n",
    "    return [c for c in candidates if textdistance.levenshtein.distance(word, c) <= max_dist]\n",
    "\n",
    "# imprime 10 ejemplos\n",
    "for w in rare[:10]:\n",
    "    s = suggest(w, cand, max_dist=1)\n",
    "    if s:\n",
    "        print(f\"{w} -> {s[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51470fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK → data/processed/preprocesado.parquet\n"
     ]
    }
   ],
   "source": [
    "# Celda 7 — Guardar preprocesado para los siguientes pasos\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "df.to_parquet(\"data/processed/preprocesado.parquet\", index=False)\n",
    "print(\"OK → data/processed/preprocesado.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98356daf",
   "metadata": {},
   "source": [
    "# Representaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef7fa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK imports.\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess, pkgutil, os\n",
    "\n",
    "def ensure(pkg, pip_name=None):\n",
    "    pip_name = pip_name or pkg\n",
    "    if pkgutil.find_loader(pkg) is None:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pip_name])\n",
    "\n",
    "ensure(\"pandas\")\n",
    "ensure(\"numpy\")\n",
    "ensure(\"scikit-learn\", \"scikit-learn\")\n",
    "ensure(\"gensim\")\n",
    "ensure(\"matplotlib\")\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "print(\"OK imports.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2251dec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10200, 10200, numpy.ndarray[slice(None, 5, None)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"data/processed/preprocesado.parquet\")\n",
    "texts  = df[\"text_norm\"].tolist()\n",
    "tokens = df[\"tokens\"].tolist()\n",
    "len(texts), len(tokens), type(tokens[0])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b19f052b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW: (10200, 157587) Voc: 157587\n",
      "TF-IDF: (10200, 1921725) Voc: 1921725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Celda 3 — BoW y TF-IDF\n",
    "bow = CountVectorizer(ngram_range=(1,1))          # unigramas\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2))        # uni+bigramas\n",
    "\n",
    "X_bow   = bow.fit_transform(texts)\n",
    "X_tfidf = tfidf.fit_transform(texts)\n",
    "\n",
    "print(\"BoW:\",   X_bow.shape,   \"Voc:\", len(bow.vocabulary_))\n",
    "print(\"TF-IDF:\", X_tfidf.shape, \"Voc:\", len(tfidf.vocabulary_))\n",
    "\n",
    "# Guardar artefactos\n",
    "import joblib\n",
    "joblib.dump(bow,   \"models/bow_vectorizer.joblib\")\n",
    "joblib.dump(tfidf, \"models/tfidf_vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bd65a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-ocurrencias: (20000, 20000) | PPMI: (20000, 20000) | Vocab: 20000\n",
      "OK → cooc/ppmi guardados.\n"
     ]
    }
   ],
   "source": [
    "# Celda 4 — Co-ocurrencia (ventana) + PPMI — FIX\n",
    "import numpy as np, pickle, os, ast\n",
    "from collections import Counter\n",
    "\n",
    "# Asegurar que 'tokens' es lista de listas (p.ej., tras cargar desde CSV/Parquet)\n",
    "if isinstance(tokens[0], str):  # venían como texto tipo \"['hola','mundo']\"\n",
    "    tokens = [ast.literal_eval(t) for t in tokens]\n",
    "\n",
    "def cooc_matrix(tokenized_docs, window=4, min_count=5, top_k=None):\n",
    "    # 1) vocabulario por frecuencia\n",
    "    vocab_counts = Counter(t for doc in tokenized_docs for t in doc)\n",
    "    items = [(w,c) for w,c in vocab_counts.items() if c >= min_count]\n",
    "    # 2) limitar vocabulario a top_k más frecuentes (evita OOM)\n",
    "    if top_k is not None and len(items) > top_k:\n",
    "        items = sorted(items, key=lambda x: -x[1])[:top_k]\n",
    "    # 3) índices\n",
    "    idx = {w:i for i,(w,_) in enumerate(sorted(items, key=lambda x: -x[1]))}\n",
    "    V = len(idx)\n",
    "    C = np.zeros((V, V), dtype=np.float32)\n",
    "    # 4) recorrer contexto por ventana\n",
    "    for doc in tokenized_docs:\n",
    "        L = len(doc)\n",
    "        for i, t in enumerate(doc):\n",
    "            if t not in idx: \n",
    "                continue\n",
    "            wi = idx[t]\n",
    "            left = max(0, i - window)\n",
    "            right = min(L, i + window + 1)\n",
    "            # izquierda\n",
    "            for u in doc[left:i]:\n",
    "                if u in idx:\n",
    "                    C[wi, idx[u]] += 1.0\n",
    "            # derecha\n",
    "            for u in doc[i+1:right]:\n",
    "                if u in idx:\n",
    "                    C[wi, idx[u]] += 1.0\n",
    "    return C, idx\n",
    "\n",
    "def ppmi(C, eps=1e-8):\n",
    "    total = C.sum()\n",
    "    if total == 0:\n",
    "        return C\n",
    "    pi = C.sum(axis=1, keepdims=True)\n",
    "    pj = C.sum(axis=0, keepdims=True)\n",
    "    pij = C / (total + eps)\n",
    "    denom = (pi @ pj) / (total**2 + eps)\n",
    "    with np.errstate(divide='ignore'):\n",
    "        pmi = np.log((pij + eps) / (denom + eps))\n",
    "    return np.maximum(0.0, pmi)\n",
    "\n",
    "# Ajusta top_k si te da memoria (20k es razonable en laptop)\n",
    "C, idx = cooc_matrix(tokens, window=4, min_count=5, top_k=20000)\n",
    "X_ppmi = ppmi(C)\n",
    "\n",
    "print(\"Co-ocurrencias:\", C.shape, \"| PPMI:\", X_ppmi.shape, \"| Vocab:\", len(idx))\n",
    "\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "np.save(\"data/processed/cooc.npy\", C)\n",
    "np.save(\"data/processed/ppmi.npy\", X_ppmi)\n",
    "with open(\"data/processed/cooc_idx.pkl\", \"wb\") as f:\n",
    "    pickle.dump(idx, f)\n",
    "print(\"OK → cooc/ppmi guardados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b8a2275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs totales: 10200 | docs válidos: 10200\n",
      "Ejemplo doc: ['valladolid', 'misteriosa', 'titulo', 'nuevo', 'libro', 'acaba', 'publicar', 'editorial', 'almuzara', 'trata', 'obra', 'investigador']\n",
      "Vocab size: 50606\n",
      "▶️  epoch 1/5\n",
      "▶️  epoch 2/5\n",
      "▶️  epoch 3/5\n",
      "▶️  epoch 4/5\n",
      "▶️  epoch 5/5\n",
      "Doc-embeddings: (10200, 100)\n",
      "OK → models/word2vec.model y data/processed/doc_embeddings_w2v.npy\n"
     ]
    }
   ],
   "source": [
    "# Celda 5 (FIX) — Word2Vec robusto + progreso + guardado\n",
    "import os, numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "# 1) Limpieza/validación: asegurar list[list[str]]\n",
    "def clean_docs(docs, min_len=2):\n",
    "    ok = []\n",
    "    for doc in docs:\n",
    "        # doc válido: lista/tupla -> la usamos; string -> split; otro iterable -> list(); si no, descartamos\n",
    "        if isinstance(doc, str):\n",
    "            seq = doc.split()\n",
    "        elif isinstance(doc, (list, tuple)):\n",
    "            seq = list(doc)\n",
    "        else:\n",
    "            try:\n",
    "                seq = list(doc)\n",
    "            except Exception:\n",
    "                continue\n",
    "        clean = []\n",
    "        for t in seq:\n",
    "            if t is None:\n",
    "                continue\n",
    "            # saltar anidados raros (listas dentro de listas, arrays, dicts, sets)\n",
    "            if isinstance(t, (list, tuple, np.ndarray, dict, set)):\n",
    "                continue\n",
    "            s = str(t).strip()\n",
    "            if s:\n",
    "                clean.append(s)\n",
    "        if len(clean) >= min_len:\n",
    "            ok.append(clean)\n",
    "    return ok\n",
    "\n",
    "tokens_clean = clean_docs(tokens, min_len=2)\n",
    "print(\"Docs totales:\", len(tokens), \"| docs válidos:\", len(tokens_clean))\n",
    "print(\"Ejemplo doc:\", tokens_clean[0][:12])\n",
    "\n",
    "# 2) Callback para ver progreso\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    def __init__(self): self.epoch = 0\n",
    "    def on_epoch_begin(self, model): \n",
    "        print(f\"▶️  epoch {self.epoch+1}/{model.epochs}\")\n",
    "    def on_epoch_end(self, model):\n",
    "        if model.compute_loss:\n",
    "            print(f\"   loss acumulado: {model.get_latest_training_loss():.1f}\")\n",
    "        self.epoch += 1\n",
    "\n",
    "# 3) Construir vocab + entrenar (usa compute_loss para ver el loss)\n",
    "w2v = Word2Vec(\n",
    "    vector_size=100, window=5, min_count=5,\n",
    "    workers=2, sg=1, epochs=5, seed=42,\n",
    "    compute_loss=True\n",
    ")\n",
    "\n",
    "w2v.build_vocab(tokens_clean, progress_per=10000)\n",
    "print(\"Vocab size:\", len(w2v.wv))\n",
    "\n",
    "w2v.train(\n",
    "    corpus_iterable=tokens_clean,\n",
    "    total_examples=w2v.corpus_count,\n",
    "    epochs=w2v.epochs,\n",
    "    report_delay=10,\n",
    "    callbacks=[EpochLogger()]\n",
    ")\n",
    "\n",
    "# 4) Vector por documento (promedio)\n",
    "def doc_avg_vecs(tokenized_docs, model):\n",
    "    dim = model.wv.vector_size\n",
    "    out = []\n",
    "    for doc in tokenized_docs:\n",
    "        vecs = [model.wv[w] for w in doc if w in model.wv]\n",
    "        out.append(np.mean(vecs, axis=0) if len(vecs) else np.zeros(dim))\n",
    "    return np.vstack(out)\n",
    "\n",
    "X_w2v = doc_avg_vecs(tokens_clean, w2v)\n",
    "print(\"Doc-embeddings:\", X_w2v.shape)\n",
    "\n",
    "# 5) Guardar artefactos\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "w2v.save(\"models/word2vec.model\")\n",
    "np.save(\"data/processed/doc_embeddings_w2v.npy\", X_w2v)\n",
    "print(\"OK → models/word2vec.model y data/processed/doc_embeddings_w2v.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c52d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 6 — Visualizaciones (PCA TF-IDF, t-SNE Word2Vec)\n",
    "# Muestras para no explotar RAM/tiempo\n",
    "n1 = min(2000, X_tfidf.shape[0])\n",
    "n2 = min(2000, X_w2v.shape[0])\n",
    "\n",
    "# PCA sobre TF-IDF (requiere denso en la muestra)\n",
    "X_tfidf_sample = X_tfidf[:n1].toarray()\n",
    "pca = PCA(n_components=2, random_state=42).fit_transform(X_tfidf_sample)\n",
    "plt.figure()\n",
    "plt.scatter(pca[:,0], pca[:,1], s=5)\n",
    "plt.title(\"PCA (TF-IDF) — muestra\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.tight_layout(); plt.savefig(\"figures/pca_tfidf.png\", dpi=160)\n",
    "plt.show()\n",
    "\n",
    "# t-SNE sobre Word2Vec (más lento; usa n2 pequeño si tu PC es modesta)\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, init=\"random\").fit_transform(X_w2v[:n2])\n",
    "plt.figure()\n",
    "plt.scatter(tsne[:,0], tsne[:,1], s=5)\n",
    "plt.title(\"t-SNE (Word2Vec doc avg) — muestra\")\n",
    "plt.xlabel(\"t1\"); plt.ylabel(\"t2\")\n",
    "plt.tight_layout(); plt.savefig(\"figures/tsne_w2v.png\", dpi=160)\n",
    "plt.show()\n",
    "\n",
    "print(\"Figuras guardadas en ./figures/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebfbb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 7 — Resumen rápido de lo logrado\n",
    "print({\n",
    "    \"bow_shape\":   tuple(X_bow.shape),\n",
    "    \"tfidf_shape\": tuple(X_tfidf.shape),\n",
    "    \"ppmi_shape\":  tuple(X_ppmi.shape),\n",
    "    \"w2v_docs\":    X_w2v.shape[0],\n",
    "    \"w2v_dim\":     X_w2v.shape[1],\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
